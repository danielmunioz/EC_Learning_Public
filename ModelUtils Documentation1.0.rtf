{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Consolas;}{\f1\fnil Consolas;}{\f2\fnil\fcharset0 Calibri;}}
{\colortbl ;\red204\green120\blue50;\red255\green198\blue109;\red169\green183\blue198;\red98\green151\blue85;\red104\green151\blue187;\red136\green136\blue198;\red106\green135\blue89;\red128\green128\blue128;\red170\green73\blue38;\red0\green0\blue255;}
{\*\generator Riched20 10.0.17134}\viewkind4\uc1 
\pard\box\brdrdash\brdrw0 \sa200\sl276\slmult1\cf1\f0\fs20 Model_Utils Documentation v1.0\f1\lang9\par
\par
def \cf2 load_nonFlare\cf3 (dataSet\cf1 , \cf3 window_length\cf1 , \cf3 length\cf1 , \cf3 is_dir=\cf1 False\cf3 ):\line     \cf4\i """\line     Extracts and loads non-flare files\f0\lang1033  from a directory or an index of files.\f1\lang9\line\line     \line     \b :param\b0  dataSet:  Directory or list containing non-flare files\line     \b :param\b0  window_length: Integer, Minimum distance to slice over time axis\line     \b :param\b0  length: Integer, Number of examples to be extracted\line     \b :param\b0  is_dir: Boolean,True oif we are using a directory, false if it is a list\line     \b :return\b0 : Lists, main_X and main_Y, containing the sliced data and their labels respectively\line     """\line\line     \cf3\i0 main_X = []\line     here = \cf5 0\line\line     \cf1 if \cf3 is_dir:\line         onlyfiles = [f \cf1 for \cf3 f \cf1 in \cf3 listdir(dataSet) \cf1 if \cf3 isfile(join(dataSet\cf1 , \cf3 f))]\line         onlyfiles = np.random.permutation(onlyfiles)\line\line         \cf1 for \cf3 index \cf1 in \cf6 map\cf3 (\cf1 lambda \cf3 x: dataSet + \cf7 '\cf1\\\\\cf7 ' \cf3 + x\cf1 , \cf3 onlyfiles):\line\line             data = fits.open(index)[\cf5 0\cf3 ].data\line\line             \cf1 if \cf3 data.shape[\cf5 0\cf3 ] < \cf5 200\cf3 :\line                 data = doubler(data)\line\line             slides = stack_window(data\cf1 , \cf3 window_length)\line\line             main_X.append(slides)\line\line             here += np.shape(slides)[\cf5 0\cf3 ]\line             \cf1 if \cf3 here >= length:\line                 \cf1 break\line\line         \cf3 main_X = np.vstack(main_X)\line         main_Y = np.full((\cf6 len\cf3 (main_X)\cf1 ,\cf3 )\cf1 , \cf7 '0'\cf3 )\line\line         \cf1 return \cf3 main_X\cf1 , \cf3 main_Y\line\line     \cf1 else\cf3 :\line         \cf1 for \cf3 index \cf1 in \cf3 dataSet:\line             data = fits.open(index)[\cf5 0\cf3 ].data\line\line             \cf1 if \cf3 data.shape[\cf5 0\cf3 ] < \cf5 200\cf3 :\line                 data = doubler(data)\line\line             slides = stack_window(data\cf1 , \cf3 window_length)\line\line             main_X.append(slides)\line\line             here += np.shape(slides)[\cf5 0\cf3 ]\line             \cf1 if \cf3 here >= length:\line                 \cf1 break\line\line         \cf3 main_X = np.vstack(main_X)\line         main_Y = np.full((\cf6 len\cf3 (main_X)\cf1 ,\cf3 )\cf1 , \cf7 '0'\cf3 )\line\line         \cf1 return \cf3 main_X\cf1 , \cf3 main_Y\par
\cf1\par
\par
def \cf2 load_Flare\cf3 (dataframe\cf1 , \cf3 window_length\cf1 , \cf3 bg_subtract=\cf1 False\cf3 ):\line\line     \cf4\i """\line     Loads data from dataframe that CONTAINS FLARES, slicing each element over time by "window_lenght"\line     It normalizes the frequency range to a value of 200 by defaul\f0\lang1033 t \f1\lang9 (\f0\lang1033 because it\f1\lang9  is the max value found so far) \par
\line     IF using already subtracted data\f0\lang1033 , please use\f1\lang9  False\f0\lang1033  for bg_subtract\f1\lang9  by default\line\line     \b :param\b0  dataframe: pandas dataframe, in base format\line     \b :param\b0  window_length: Integer, Minimum distance to slice over time axis\line     \b :param\b0  bg_subtract: Boolean, used to apply the standard bg_subt method from sunpy while slicing\line     \b :return\b0 : Lists, main_X and main_Y, containing the sliced data and their labels respectively\line     """\line\line\line     \cf3\i0 main_X = []\line\line     \cf1 for \cf3 index\cf1 , \cf3 elemen \cf1 in \cf3 dataframe.iterrows():\line\line         flare = get_flare(dataframe\cf1 , \cf3 index\cf1 , \cf3 window_length\cf1 , \cf3 bg_subtract)\line         \cf1 if \cf3 flare.shape[\cf5 0\cf3 ] < \cf5 200\cf3 :\line             flare = doubler(flare)\line\line         slides = stack_window(flare\cf1 , \cf3 window_length)\line\line         main_X.append(slides)\line\line     main_X = np.vstack(main_X)\line     main_Y = np.full((\cf6 len\cf3 (main_X)\cf1 ,\cf3 )\cf1 , \cf7 '1'\cf3 )\line\line     \cf1 return \cf3 main_X\cf1 , \cf3 main_Y\par

\pard\box\brdrdash\brdrw0 \sa200\sl276\slmult1\cf0\par
\cf1 def \cf2 get_flare\cf3 (dataframe\cf1 , \cf3 index\cf1 , \cf3 window_length\cf1 , \cf3 bg_subtract=\cf1 False\cf3 ):\line\line     \cf4\i """\line     Extracts Flare from joined Fits file using info from \f0\lang1033 a \f1\lang9 dataframe\f0\lang1033 , using starting and ending positions relative to the file. \par
\f1\lang9\line     \b :param\b0  dataframe:\f0\lang1033  pandas dataframe\f1\lang9\line     \b :param\b0  index:\f0\lang1033  position of the file in the dataframe\f1\lang9\line     \b :param\b0  window_length:\f0\lang1033  length of the window to crop\f1\lang9\line     \b :param\b0  bg_subtract:\f0\lang1033  Boolean, if true a standard background subtraction is applied\f1\lang9\line     \b :return\b0 :\f0\lang1033  A data patch containing the flare from the file\f1\lang9\line     """\line     \cf3\i0 file_here = fits.open(dataframe.loc[index][\cf7 'remarks'\cf3 ])\line\line     start = datetime.datetime.strptime(dataframe.loc[index][\cf7 'start'\cf3 ]\cf1 , \cf3 fmt)\line     end = datetime.datetime.strptime(dataframe.loc[index][\cf7 'end'\cf3 ]\cf1 , \cf3 fmt)\line\line     time_obs = datetime.datetime.strptime(file_here[\cf5 0\cf3 ].header[\cf7 'TIME-OBS'\cf3 ][:\cf5 8\cf3 ]\cf1 , \cf7 '%H:%M:%S'\cf3 )\line     time_end = datetime.datetime.strptime(file_here[\cf5 0\cf3 ].header[\cf7 'TIME-END'\cf3 ]\cf1 , \cf7 '%H:%M:%S'\cf3 )\line\line     lenght = file_here[\cf5 0\cf3 ].data.shape[\cf5 1\cf3 ]\line     time_window = time_end - time_obs\line\line     \cf8 # Trying to normalize data\line     \cf1 if \cf3 time_obs > start:\line         sec_toAdd = time_obs - start\line         start = start + datetime.timedelta(\cf9 seconds\cf3 =sec_toAdd.seconds)\line\line     \cf8 # with time obs\line     \cf3 start_seconds = start - time_obs\line     end_seconds = end - time_obs\line\line     steps_start = \cf6 int\cf3 (start_seconds.seconds / time_window.seconds * lenght)\line     steps_end = \cf6 int\cf3 (end_seconds.seconds / time_window.seconds * lenght)\line     f_length = steps_end - steps_start\line\line     \cf1 if \cf3 f_length < window_length:\line         steps_start\cf1 , \cf3 steps_end = length_adjustment(f_length\cf1 , \cf3 lenght\cf1 , \cf3 steps_start\cf1 , \cf3 steps_end\cf1 , \cf3 window_length)\line\line     \cf8 # Getting patch\line     \line     \cf1 if \cf3 bg_subtract:\line         flare_patch = standard_subtract(file_here[\cf5 0\cf3 ].data[:\cf1 , \cf3 steps_start:steps_end])\line     \cf1 else\cf3 :\line         flare_patch = file_here[\cf5 0\cf3 ].data[:\cf1 , \cf3 steps_start:steps_end]\line\line     file_here.close()\line\line     \cf1 return \cf3 flare_patch\par
\cf0\par

\pard\box\brdrdash\brdrw0 \sa200\sl276\slmult1\cf1 def \cf2 length_adjustment\cf3 (f_lenght\cf1 , \cf3 lenght\cf1 , \cf3 start_position\cf1 , \cf3 end_position\cf1 , \cf3 window_size):\line\cf4\i """\line\f0\lang1033 Adjust length depending of the position of the flare respecting of the file, this means there are some cases where the flares are relatively short (in the time axis) and they are found in one of the corners of our file, so we want to adjust the starting and ending point in order crop the flare properly from these edges.\par
\tab\b\f1\lang9 :param\b0  f_length: Length of the flare\line\f0\lang1033\tab\b\f1\lang9 :param\b0  length: length of the file\line\f0\lang1033\tab\b\f1\lang9 :param\b0  start_position: Start position of the flare\line\f0\lang1033\tab\b\f1\lang9 :param\b0  end_position: End position of the flare\line\f0\lang1033\tab\b\f1\lang9 :param\b0  window_size: Size of the slicing window\line\f0\lang1033\tab\b\f1\lang9 :return\b0 : Positions to crop the file\line """\par

\pard\box\brdrdash\brdrw0 \sa200\sl276\slmult1     \cf3\i0 true_add = window_size - f_lenght\line\line     addL = \cf6 int\cf3 (true_add / \cf5 2\cf3 )\line     addR = np.abs(\cf6 int\cf3 (true_add / \cf5 2\cf3 ) - true_add)\line\line     new_end = end_position + addR\line     new_start = start_position - addL\line\line     \cf1 if \cf3 new_end <= lenght \cf1 and \cf3 new_start >= \cf5 0\cf3 :\line         \cf1 return \cf3 new_start\cf1 , \cf3 new_end\line\line     \cf1 if \cf3 new_end > lenght:\line         \cf8 # We're in the right\line         \cf3 new_start = start_position - true_add\line         new_end = end_position\line         \cf1 return \cf3 new_start\cf1 , \cf3 new_end\line\line     \cf1 if \cf3 new_start < \cf5 0\cf3 :\line         \cf8 # We're in the left\line         \cf3 new_start = start_position\line         new_end = new_end + true_add\line         \cf1 return \cf3 new_start\cf1 , \cf3 new_end\line\line     \cf1 return \cf3 new_start\cf1 , \cf3 new_end\line\par
\cf0\f0\lang1033 These three methods are just a re-implementation of the original bg_subtract found in the radiospectra package\f1\lang9\par
\cf1\f0\lang1033 #\f1 def \cf2 auto_find_background\cf3 (fits_data\cf1 , \cf3 amount=\cf5 0.05\cf3 ):\line     data = fits_data\line     tmp = (data - np.average(fits_data\cf1 , \cf5 1\cf3 ).reshape(fits_data.shape[\cf5 0\cf3 ]\cf1 , \cf5 1\cf3 ))\line     sdevs = np.asarray(np.std(tmp\cf1 , \cf5 0\cf3 ))\line\line     cand = \cf6 sorted\cf3 (\cf6 range\cf3 (fits_data.shape[\cf5 1\cf3 ])\cf1 , \cf9 key\cf3 =\cf1 lambda \cf3 y: sdevs[y])\line     \cf1 return \cf3 cand[:\cf6 max\cf3 (\cf5 1\cf1 , \cf6 int\cf3 (amount * \cf6 len\cf3 (cand)))]\line\line\cf1 def \cf2 auto_const_bg\cf3 (fits_data):\line     realcand = auto_find_background(fits_data)\line     bg = np.average(fits_data[:\cf1 , \cf3 realcand]\cf1 , \cf5 1\cf3 )\line     \cf1 return \cf3 bg.reshape(fits_data.shape[\cf5 0\cf3 ]\cf1 , \cf5 1\cf3 )\line\line\cf1 def \cf2 standard_subtract\cf3 (fits_data):\line     \cf1 return \cf3 fits_data - auto_const_bg(fits_data)\par
\cf0\f0\lang9\par
\cf1 def \cf2 doubler\cf3 (data_here):\line     \cf4\i """\line     Doubles the data over Y axis, this is done by duplicating each row down.\par
Example:\par
\tab 64 65 64 65 67 68 67 67 67 67 69\par
\tab 57 58 59 50 58 56 54 53 55 56 57\par
It is converted to:\par
\tab 64 65 64 65 67 68 67 67 67 67 69\par
\tab 64 65 64 65 67 68 67 67 67 67 69\par
\tab 57 58 59 50 58 56 54 53 55 56 57\par
\tab 57 58 59 50 58 56 54 53 55 56 57\par
This is done as a fix for the "Y axis length problem" in wich the frequency axis of the files did not match,(size 100 and 200), so we dulicate the values in order to get a fixed size in the Y axis.\par
\line     \b :param\b0  data_here: section of the data to double\line     \b :return\b0 : new doubled data\line     """\line     \cf3\i0 before = np.vstack([data_here[\cf5 0\cf3 ]\cf1 , \cf3 data_here[\cf5 0\cf3 ]])\line\line     \cf1 for \cf3 elemen \cf1 in \cf6 range\cf3 (data_here.shape[\cf5 0\cf3 ] - \cf5 1\cf3 ):\line         elemen += \cf5 1\line\line         \cf3 here = np.vstack([data_here[elemen]\cf1 , \cf3 data_here[elemen]])\line\line         before = np.vstack([before\cf1 , \cf3 here])\line\line     \cf1 return \cf3 before\line\par
\cf0\f1\par
\cf1 def \cf2 stack_window\cf3 (data\cf1 , \cf3 window_lengt\f0\lang1033 h\f1\lang9 ):\line     \cf4\i """\line     \f0\lang1033 Crops and stacks the data from a file, A crop is made with a length of "window_length".\par
\f1\lang9\line     \b :param\b0  data:\f0\lang1033  Data from a previously opened fits file.\f1\lang9\line     \b :param\b0  window_lenght:\f0\lang1033  length of the window to crop.\f1\lang9\line     \b :return\b0 :\f0\lang1033  List of Stacked windows.\f1\lang9\line     """\line     \cf3\i0 time_step = window_lengt\f0\lang1033 h\f1\lang9\line     windows = []\line\line     \cf1 while \cf3 time_step <= data.shape[\cf5 1\cf3 ]:\line         x_start = time_step - window_lengt\f0\lang1033 h\f1\lang9\line         x_end = time_step\line\line         window = data[:\cf1 , \cf3 x_start:x_end]\line         time_step = time_step + window_lengt\f0\lang1033 h\f1\lang9\line         windows.append(window)\line\line     \cf1 return \cf3 windows\par
\par
\cf1 def \cf2 load_nonFlare\cf3 (dataSet\cf1 , \cf3 window_length\cf1 , \cf3 length\cf1 , \cf3 is_dir=\cf1 False\cf3 ):\line     \cf4\i """\line     Extracts and loads non-flare files.\line\line     If normalize, window_height should be 200 (BC is the max value found so far) so we set the frequency bin to 200\line\line\line     \b :param\b0  dataSet:  Directory or list containing non-flare files\line     \b :param\b0  window_length: Integer, Minimum distance to slice over time axis\line     \b :param\b0  length: Integer, Number of examples to be extracted\line     \b :param\b0  is_dir: Boolean,True oif we are using a directory, false if it is a list\line     \b :return\b0 : Lists, main_X and main_Y, containing the sliced data and their labels respectively\line     """\line\line     \cf3\i0 main_X = []\line     here = \cf5 0\line\line     \cf1 if \cf3 is_dir:\line         onlyfiles = [f \cf1 for \cf3 f \cf1 in \cf3 listdir(dataSet) \cf1 if \cf3 isfile(join(dataSet\cf1 , \cf3 f))]\line         onlyfiles = np.random.permutation(onlyfiles)\line\line         \cf1 for \cf3 index \cf1 in \cf6 map\cf3 (\cf1 lambda \cf3 x: dataSet + \cf7 '{\cf1{\field{\*\fldinst{HYPERLINK "\\\\\\\\'"}}{\fldrslt{\\\\\cf7 '\ul0\cf0}}}}\f1\fs20  \cf3 + x\cf1 , \cf3 onlyfiles):\line\line             data = fits.open(index)[\cf5 0\cf3 ].data\line\line             \cf1 if \cf3 data.shape[\cf5 0\cf3 ] < \cf5 200\cf3 :\line                 data = doubler(data)\line\line             slides = stack_window(data\cf1 , \cf3 window_length)\line\line             main_X.append(slides)\line\line             here += np.shape(slides)[\cf5 0\cf3 ]\line             \cf1 if \cf3 here >= length:\line                 \cf1 break\line\line         \cf3 main_X = np.vstack(main_X)\line         main_Y = np.full((\cf6 len\cf3 (main_X)\cf1 ,\cf3 )\cf1 , \cf7 '0'\cf3 )\line\line         \cf1 return \cf3 main_X\cf1 , \cf3 main_Y\line\line     \cf1 else\cf3 :\line         \cf1 for \cf3 index \cf1 in \cf3 dataSet:\line             data = fits.open(index)[\cf5 0\cf3 ].data\line\line             \cf1 if \cf3 data.shape[\cf5 0\cf3 ] < \cf5 200\cf3 :\line                 data = doubler(data)\line\line             slides = stack_window(data\cf1 , \cf3 window_length)\line\line             main_X.append(slides)\line\line             here += np.shape(slides)[\cf5 0\cf3 ]\line             \cf1 if \cf3 here >= length:\line                 \cf1 break\line\line         \cf3 main_X = np.vstack(main_X)\line         main_Y = np.full((\cf6 len\cf3 (main_X)\cf1 ,\cf3 )\cf1 , \cf7 '0'\cf3 )\line\line         \cf1 return \cf3 main_X\cf1 , \cf3 main_Y\line\par
\par
\cf1 def \cf2 split_List\cf3 (directory\cf1 , \cf3 percentage):\line     \cf4\i """\line     Splits List extracted from \f0\lang1033 directory \f1\lang9 and, randomly, divides it into Train and eval\f0\lang1033  set\f1\lang9  by percentage\line\line     \b :param\b0  directory: String, Directory of the elements\line     \b :param\b0  percentage: Float, value from 0 to 1 used to split the list\line     \b :return\b0 : train_set, eval_set\line     """\line     \cf3\i0 onlyfiles = [f \cf1 for \cf3 f \cf1 in \cf3 listdir(directory) \cf1 if \cf3 isfile(join(directory\cf1 , \cf3 f))]\line     onlyfiles = np.random.permutation(onlyfiles)\line\line     train_length = \cf6 int\cf3 (np.shape(onlyfiles)[\cf5 0\cf3 ] * percentage)\line\line     train_set = onlyfiles[\cf5 0\cf3 :train_length]\line     eval_set = onlyfiles[train_length:np.shape(onlyfiles)[\cf5 0\cf3 ]]\line\line     train_set = \cf6 list\cf3 (\cf6 map\cf3 (\cf1 lambda \cf3 x: directory + \cf7 '/' \cf3 + x\cf1 , \cf3 train_set))\line     eval_set = \cf6 list\cf3 (\cf6 map\cf3 (\cf1 lambda \cf3 x: directory + \cf7 '/' \cf3 + x\cf1 , \cf3 eval_set))\line\line     \cf1 return \cf3 train_set\cf1 , \cf3 eval_set\line\line\line\cf1 def \cf2 split\cf3 (dataset\cf1 , \cf3 percentage):\line     \cf4\i """\line     Splits dataset and, random\f0\lang1033 l\f1\lang9 y, divides it into Train and eval by percentage\line\line     \b :param\b0  dataset: Pandas Dataframe, Directory of the elements\line     \b :param\b0  percentage: Float, value from 0 to 1 used to split the list\line     \b :return\b0 : train_set, eval_set\line     """\line     \cf3\i0 train_length = \cf6 int\cf3 (dataset.shape[\cf5 0\cf3 ] * percentage)\line     train_set = pd.DataFrame(\cf9 columns\cf3 =dataset.columns)\line     eval_set = pd.DataFrame(\cf9 columns\cf3 =dataset.columns)\line\line     permut = np.random.permutation(dataset.shape[\cf5 0\cf3 ])\line     train_index = permut[\cf5 0\cf3 :train_length]\line     eval_index = permut[train_length:dataset.shape[\cf5 0\cf3 ]]\line\line     train_set = train_set.append(dataset.loc[train_index])\line     eval_set = eval_set.append(dataset.loc[eval_index])\line\line     \cf1 return \cf3 train_set\cf1 , \cf3 eval_set\par
\par

\pard\box\brdrdash\brdrw0 \ri-360\sa200\sl276\slmult1\cf0\par

\pard\sa200\sl276\slmult1\f2\fs22\par
}
 